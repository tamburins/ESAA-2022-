{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMThwrfVhsoewt7Awofuhbw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamburins/ESAA-2022-/blob/main/ESAA_5_5(512~516%2C529~543).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 06 토픽 모델링(topic modeling) - 20 뉴스그룹\n",
        "\n",
        "문서 집합에 숨어있는 주제를 찾아내는 것으로 더 함축적인 의미로 문장을 요약하는 사람의 토픽모델링에 비해 머신러닝 기반의 토픽모델은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출합니다.\n",
        "\n",
        "머신러닝 기반의 토픽모델링에 자주 사용되는 기법은 lsa와 lda로 이 절에서는 lda만을 이용해 토픽 모델링을 수행하겠다. 이는 차원축소의 lda와는 다른 알고리즘이다. 토픽모델링은 텍스트분류에서 소개한 20뉴스그룹 데이터세트를 이용해 적용하고자 한다.\n",
        "\n",
        "여러 주제중 모토사이클, 야구, 그래픽스, 윈도우, 중동, 기독교, 전자공학, 의학의 8개 주제를 추출하고 이들 텍스트에 lda 기반의 토픽 모델링을 적용해보고자한다.\n",
        "\n",
        "먼저 lda 토픽 모델링을 위해 fetch_20newsgroups()는 categories 파라미터를 통해 필요한 주제만 필터링해 추출하고 추출된 텍스트를 count 기반으로 벡터화 변환한다. lda는 count 기반의 벡터화만 사용하며 max_ features1000통해 word 피처의 개수를 제한하고 ngram_range는 (1,2)로 설정하여 피처벡터화 변환하겠다"
      ],
      "metadata": {
        "id": "-V0QbM7W0hVg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvnhtZjo0TAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5126d393-b4d4-4d53-cd72-8fcb840a1ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer shape:  (7862, 1000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# 모토사이클, 야구, 그래픽스, 윈도우즈 중동 기독교 전자공학 의학 8개 주제 추출\n",
        "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x','talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med']\n",
        "# 위에서 cats 변수로 기재된 카테고리만 추출하고 fetch_20groups에 cats 입력\n",
        "news_df = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'),\n",
        "                            categories=cats, random_state=0)\n",
        "#lda is adapted in count based vectoreization\n",
        "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english',\n",
        "                             ngram_range=(1,2))\n",
        "fect_vect=count_vect.fit_transform(news_df.data)\n",
        "print('CountVectorizer shape: ', fect_vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "카운트 벡터라이저 객체변수인 featvet모두 7000여개의 문서가 1000여개의 피처로 구성된 행렬데이터이다. 이렇게 벡터화된 데이터세트를 기반을 lda 토픽 모델링을 수행한다. 토픽의 개수는 cats 개수인 8개로 Latentdirichletallocation 클래스의 ncomponents파라미터를 이용해 토픽개수를 조정한다."
      ],
      "metadata": {
        "id": "Y2oKJZH3mXg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
        "lda.fit(fect_vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "GKJQBLE-mpC2",
        "outputId": "6c72336e-372a-4af0-9db8-1011e208b2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(n_components=8, random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lda.fit을 수행하면 객체는 component 속성값을 갖는데 이는 개별 토픽별로 word 피처가 얼마나 많은지 그 토픽에 할당됐는지에 대한 수치를 갖는다. 높은 갑일 수록 해당 word피처는 토픽의 중심 word가 된다."
      ],
      "metadata": {
        "id": "azAERNt_mxOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda.components_.shape)\n",
        "lda.components_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkx046JCm8O0",
        "outputId": "fdc994d2-659a-45bb-f2f9-53dfd06e8056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 1000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
              "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
              "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
              "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
              "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
              "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
              "       ...,\n",
              "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
              "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
              "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
              "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
              "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
              "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "componetns는 8행 4000열로 구성되어있는데, 이는 8개의 토픽별로 1000개의 피ㅓ가 해당 토픽별로 연관도 값을 갖는 것을 의미한다. 이러한 components 값만으로 는 각 토픽별 word 연관도를 보기가 어려우니 display topics funnction을 생성하여 토픽별 연관도가 높은 순으로 word를 나열해보고자 함"
      ],
      "metadata": {
        "id": "pHfwKptsnBj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "  for topic_index, topic in enumerate(model.components_):\n",
        "    print('Topic #', topic_index)\n",
        "\n",
        "    #coponentsarray에서 가장 값이 큰 순으로 정ㄹ려시 그값의 array인덱스 반환\n",
        "    topic_word_indexes = topic.argsort()[::-1]\n",
        "    topic_indexes = topic_word_indexes[:no_top_words]\n",
        "\n",
        "    #top indexes 대상인 인덱스별로 feature names에 해당하는 word feature 추출 후 join으로 concat\n",
        "    feature_concat = ''.join([feature_names[i] for i in topic_indexes])\n",
        "    print(feature_concat)\n",
        "\n",
        "# in countvectorizer extract names of all word by get_feature_names()\n",
        "feature_names = count_vect.get_feature_names_out()\n",
        "#메소드 이름 변경(?)\n",
        "\n",
        "#top 15 of 연관도 for each topic\n",
        "display_topics(lda, feature_names,15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk414BMfnBV2",
        "outputId": "d3f6b0fb-ef4d-45cd-ea46-243449ea0816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic # 0\n",
            "year10gamemedicalhealthteam1220diseasecancer1993gamesyearspatientsgood\n",
            "Topic # 1\n",
            "donjustlikeknowpeoplesaidthinktimevedidnrightgoingsayllway\n",
            "Topic # 2\n",
            "imagefilejpegprogramgifimagesoutputformatfilescolorentry00usebit03\n",
            "Topic # 3\n",
            "likeknowdonthinkusedoesjustgoodtimebookreadinformationpeopleusedpost\n",
            "Topic # 4\n",
            "armenianisraelarmeniansjewsturkishpeopleisraelijewishgovernmentwardos dosturkeyarabarmenia000\n",
            "Topic # 5\n",
            "educomavailablegraphicsftpdatapubmotifmailwidgetsoftwaremitinformationversionsun\n",
            "Topic # 6\n",
            "godpeoplejesuschurchbelievechristdoeschristiansaythinkchristiansbiblefaithsinlife\n",
            "Topic # 7\n",
            "usedosthankswindowsusingwindowdoesdisplayhelplikeproblemserverneedknowrun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "topic 0의 경우 명확하지 않고 일반적인 단어가 주를 이루며 1의 경우 명확하게 컴퓨터그래픽스 단어가 추출되었다. 2는 기독교 관련 주제어가, 3은 의학에 관한 주제어가, 4는 윈도우 운영체제와 관련한 주제어가 추출되었으나 5는 일반적인 단어로 주제어가 추출되었다. 6은 중동분쟁관련어 7은 애매하지만 우니도우 운영체제와 관한 주제어가 일부 추출되었다. 0,5,7의 토픽에서 애매한 주제어가 추출되었다."
      ],
      "metadata": {
        "id": "-sd0XcVhoT6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 문서 유사도\n",
        "( 528~\n",
        "\n",
        "### 문서 유사도 측정 방법 - 코사인 유사도\n",
        "이는 벡터와 벡터간 유사도를 비교할 때 벡터의 크기보다는 벡터의 상호방향성이 얼마나 유사한지에 기반으로 한다.\n",
        "\n",
        "### 두 벡터 사잇각\n",
        "두 벡터의 사잇각에 따라서 상호관계는 유사하거나 관련이 없거나 아예 반대관계가 될 수 있다. 두 벡터사이 cos값은 두 벡터 a,b 의 내적이 a의 길이와 b의 길이의 크기와 두 벡터사이 코사인값으로 나타내는 것에서 기인하여, 유사도 cos을 구할 있다. 유사도는 두 벡터의 내적을 총 벡터크기의 합으로 나눈 것, 즉 내적 결과를 총 벡터의 크기로 정규화한 것이라 할 수 있다.\n",
        "\n",
        "문서를 피처벡터화 변환시 많은 희소행렬이 되기 쉬운데 이러한 희소행렬 기반에서 문서와 문서벡터간의 크기에 기반한 유사도지표는 정확도가 떨어지며, 문서가 매우 긴 경우 단어의 빈도수가 또한 많기 때문에 빈도수 기반도 어렵다\n",
        "\n",
        "간단한 문서에 대해 서로간의 문서 유사도를 코사인 유사도 기반으로 구해보자."
      ],
      "metadata": {
        "id": "amE4y46coqnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def cos_similarity(v1, v2):\n",
        "  dot_product = np.dot(v1,v2)\n",
        "  _norm = (np.sqrt(sum(np.square(v1)))*np.sqrt(sum(np.square(v2))))\n",
        "  similarity = dot_product / _norm\n",
        "\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "dhVPiVqCphJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf idf 변환\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "doc_list = ['if you take the blue pill, the story ends',\n",
        "'if you take take red pill, you stay in Wonderland',\n",
        "'if you take the red pill, I show you how dep the rabbit hole goes']\n",
        "\n",
        "tfidf_vect_simple = TfidfVectorizer()\n",
        "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
        "print(feature_vect_simple.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C_4buF3p08U",
        "outputId": "ce3711ca-5959-4b90-f0d6-3fc2dc5f9ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "변환된 행렬은 희소행렬로 앞에서 작성한 cos similarity 함수의 인자인 array로 만들기 ㅜ이해 밀집행렬로 변환한 뒤 다시 각각을 배열로 반환한다. feature_vect_dense[0]은 doc list 첫번/재 문서의 피처벡터화, feature_vect_dense[1]은 doc list 2번재 문서의 피처벡터화이다."
      ],
      "metadata": {
        "id": "fAI2YCYDrtUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with tfidf vectorizer, transform result is needed to transformation to dense\n",
        "feature_vect_dense = feature_vect_simple.todense()\n",
        "\n",
        "# firest sentence and second sentence's extraction of feature\n",
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "\n",
        "# with feature vector of first n second sentence, extract cos similarity\n",
        "similarity_simple = cos_similarity(vect1, vect2)\n",
        "print('sentence1, sentence2 cosine similarity: {:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29IC1Q7Jr-fe",
        "outputId": "4bb7036a-56fb-4cfb-9d34-714eb8a47984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence1, sentence2 cosine similarity: 0.304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get similarity with sent2,3 and sent1,3\n",
        "\n",
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect1, vect3)\n",
        "print('sentence1, sentence3 cosine similarity: {:.3f}'.format(similarity_simple))\n",
        "\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect2, vect3)\n",
        "print('sentence2, sentence3 cosine similarity: {:.3f}'.format(similarity_simple))\n",
        "# numpy.float64' can't '{0:.3f}'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyhnJJ6gsv13",
        "outputId": "8c5e0350-c7d7-4c65-cf0e-0271b10772f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence1, sentence3 cosine similarity: 0.465\n",
            "sentence2, sentence3 cosine similarity: 0.376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "사이킷런은 코사인 유사도 측정을 위해 api를 제공한다. 이를 이용해보자. 입력 파라미터는 비교 기준이 되는 문서의 피처행렬, 두번째 파라미터는 비교되는 문서의 피처행렬이다. 꼭 밀집행렬이 아니라도 가능하며 행렬 또는 배열 모두 가능하다. 앞에서처럼 굳이 변환작업이 필요하진 않다."
      ],
      "metadata": {
        "id": "yhqXS3u-surY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple)\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_7aoQsotQMh",
        "outputId": "cd5dde17-d4e6-483c-d46e-ddc8c189a1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.30396839 0.4653398 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫번째 유사도값인 1은 비교기준인 첫번째 문서자신에 대한 유사도 측정이며 두번째는 첫번째와 두번째 문서, 마지막은 첫번째와 세번째 문서 사이의 유사도 값이다. 만약 1이 거슬릴경우 비교기준 문서를 제할 수 있다."
      ],
      "metadata": {
        "id": "BC5Th8c5tk_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple[1:])\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlAZy8pUtwZr",
        "outputId": "4c56774d-3645-4c08-9f90-e0858a1ec744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.30396839 0.4653398 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPinion reveiw 데이터 세트를 이용한 문서 유사도 측정"
      ],
      "metadata": {
        "id": "GDzexCnDxw6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#####파일리딩이 전혀 안돼서 코드로만 작성합니다....\n",
        "```\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "  import nltk\n",
        "  import string\n",
        "\n",
        "# 단어 원형 추출 함수\n",
        "lemmar = WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "# 특수 문자 사전 생성: {33: None ...}\n",
        "# ord(): 아스키 코드 생성\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "# 특수 문자 제거 및 단어 원형 추출\n",
        "def LemNormalize(text):\n",
        "    # 텍스트 소문자 변경 후 특수 문자 제거\n",
        "    text_new = text.lower().translate(remove_punct_dict)\n",
        "    \n",
        "    # 단어 토큰화\n",
        "    word_tokens = nltk.word_tokenize(text_new)\n",
        "    \n",
        "    # 단어 원형 추출\n",
        "    return LemTokens(word_tokens)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OHkup4RUogdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# 단어 원형 추출 함수\n",
        "lemmar = WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "# 특수 문자 사전 생성: {33: None ...}\n",
        "# ord(): 아스키 코드 생성\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "# 특수 문자 제거 및 단어 원형 추출\n",
        "def LemNormalize(text):\n",
        "    # 텍스트 소문자 변경 후 특수 문자 제거\n",
        "    text_new = text.lower().translate(remove_punct_dict)\n",
        "    \n",
        "    # 단어 토큰화\n",
        "    word_tokens = nltk.word_tokenize(text_new)\n",
        "    \n",
        "    # 단어 원형 추출\n",
        "    return LemTokens(word_tokens)```\n",
        "\n"
      ],
      "metadata": {
        "id": "92HuM8rFpvvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "import glob, os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "path =\n",
        "all_files = glob.glob(os.path.join(path, '*.data'))\n",
        "filename_list = []\n",
        "opinion_text = []\n",
        "\n",
        "for file_ in all_files:\n",
        "  df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
        "  filename_ = file_.split('\\\\')[-1]\n",
        "  filename = filename_.split('.')[0]\n",
        "  filename_list.append(filename)\n",
        "  opinion_text.appedn(df.to_string())\n",
        "\n",
        "document_df = pd.DataFrame({'filename':'filename_list', 'opinion_text':'opinion_text'})\n",
        "tfidf_vect = TfidfVectorizer((tokenizer = LemNormalize, stop_words='english',\n",
        "                              ngram_range=(1,2), min_df=0.05, max_df=0.85))\n",
        "feature_text = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
        "\n",
        "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "document_df['cluster_label'] = cluster_label```\n",
        "\n"
      ],
      "metadata": {
        "id": "vr1iw8mip2Iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "호텔을 주제로 군집화된 문서를 이용해 특정 문서와 다른 문서간의 유사도를 알아보고자 한다. 문서를 피처벡터화 변환하고 cosine similarity를 이용해 유사도를 확인해보고자 한다.\n",
        "\n",
        "먼저 군집화된 데이터를 추출하고 이 데이터에 해당ㅎ라는 tfidf vectorizer의 데이터를 추출하고자 한다. 호텔 군집화 데이터를 기반으로 별도의 tfidf 벡텇화를 수행하는게 아니라 바로 위에서 추출할 것이다.\n",
        "\n",
        "데이터프레임 객체변수인 document Df 에서 먼저 호텔로 군집화된 문서의 인덱스를 추출한다. 이렇게 추출된 인덱스를 그대로 이용해 feature vectord에서 군집화된 문서의  피처 벡터를 추출한다."
      ],
      "metadata": {
        "id": "8Gvx9oW8NJg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# cluster label=1 인 데이터는 호텔로 군집화된 데이터. 데이터프레임에서 이를 추출\n",
        "hotel_indexes = document_df[document_df['cluster_label']==1].index\n",
        "print('hotel clustered documents dataframe index: ', hotel_indexes)\n",
        "\n",
        "# among hotel clustered datas, first data extraction, filename\n",
        "comparison_docname = document_df.iloc[hotel_indexes[0]]['filename']\n",
        "print('비교 기준 문서명 ', comparison_docname, '와 타 문서 유사도')\n",
        "\n",
        "'''document df에서 추출된 index 객체를 feature vec로 입력해 호텔 군집화된 feature vect 추출\n",
        "이를 이용해 호텔로 군집화된 문서 중 첫번쨰 문서와 다른 문서간의 코사인 유사도 추출''''\n",
        "similariy_pair = cosine_similarity(feature_vect[hotel_indexes[0]], feature_vect[hotel_indexes])\n",
        "print(similarity_pair)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lXPqNDWop57i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "숫자로만 표현해서는 직관적인 유사도를 알기 어려우므로 첫번째와 문서간 유사도가 높은 순으로 이를 정렬하고 시각화해보고자 함"
      ],
      "metadata": {
        "id": "VtQHlYBeSXsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 첫번째 문서와 타 문서간의 유사도가 큰 순으로 정렬한 인덱스를 추출하되 자기 자신은 제외\n",
        "sorted_index = similarity_pair.argsort()[:,::-1]\n",
        "sorted_index = sorted_inex[:,1:]\n",
        "\n",
        "#유사도가 큰 순으로 hotel indexes를 추출해 재정렬\n",
        "hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1)]\n",
        "\n",
        "# 유사도가 큰 순으로 재정렬하되 자기자신 제외\n",
        "hotel_1_sim_value = np.sort(similarity_pair.rehsape(-1)[::-1])\n",
        "hotel_1_sim_value = hotel_1_sim_value[1:]\n",
        "\n",
        "# 유사도가 큰 순으로 정렬된 인덱스와 유사도값을 이용해 파일명과 유사도값을 막대 그래프로 시가괗\n",
        "hotel_1_sim_df = pd.DataFrame()\n",
        "hotel_1_sim_df['filename'] = document_df.iloc[hotel_sorted_index]['filename']\n",
        "hotel_1_sim_df['similarity']=hotel_1_sim_value\n",
        "\n",
        "sns.barplot(x='similarity', y='filename', data=hotel_1_sim_df)\n",
        "plt.title(comparison_docname)```\n",
        "\n"
      ],
      "metadata": {
        "id": "bwfUkTxQqAE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫번째 문서인 샌프란시스코의 베스트웨스턴 호텔화장실 리뷰와 가장 비슷한 홑\b문서는 room_holiday_inn_london으로 0.5정도의 코사인 유사도를 나타낸다"
      ],
      "metadata": {
        "id": "-FbmGZxDTo4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 한글 텍스트 처리 - 네이버 영화 평점 감상분석\n",
        "\n",
        "### 한글 NLP 처리의 어려움\n",
        "한국어는 띄어쓰기와 다양한 조사로 인해 처리가 어려\b렵다. 특히 띄어쓰기의 경우 잘못될 경우 의미가 왜곡되며 조사가 다양하기 때문에 어렵다.\n",
        "\n",
        "### KoNLP 소개\n",
        "대표적인 한글 형태소 패키지로 형태소란 사전적으로 단어로서 의미를 갖는 최소단위를 의미한다. 형태소 분석이란 말뭉치를 이러한 형태소 어근 단위로 쪼개고 각 형태소에 품사태깅을 부착하는 작업을 일반적으로 지칭한다.\n",
        "\n",
        "이전에는 거의 없었으며 대부분 씨나 자바 기반 패키지로 개발되었으나 konlpy는 기존의 씨나 자바기반의 형태소엔진을 파이선 래퍼 기준으로 재작성한 패키지이다. 기존의 5개 형태소 분석모델을 모두 사용가능하나 mecab의 경우 윈도우환경에선ㄴ 구동이 불가능하다.\n",
        "\n",
        "\n",
        "konlpy는 공식 설치문서를 참조하는데 이는 자바와 씨 기반이기 때문에 자바가 먼저 설치되어 있어야 한다. \n",
        "\n",
        "jpype1을 찾아 개인 pc 환경에 맞는 것을 찾아 모듈을 설치한다3"
      ],
      "metadata": {
        "id": "OWJxUJbzTzdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#C:\\Users\\chkwon\\conda install c- conda-forge jpype1"
      ],
      "metadata": {
        "id": "xEPHc0L1TySG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아나콘다를 사용하지 않고도 pip으로도 셋업이 가능한데 요즘 버전에 맞게 파일명을 변경해야 한다."
      ],
      "metadata": {
        "id": "EY1UH9doU-tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#C:\\Users\\chkwon> pip install --upgrade pip\n",
        "#C:\\Users \\chkwon> pip install JPype1-0.6.3-cp36-cp36m-win_amd64.wh1"
      ],
      "metadata": {
        "id": "kpdlNDZtVIfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 자바환경설정이 필요한데 자바홈 설정이 어려운 경우 윈도우 환경설치문서의 java home 설정하기를 클릭하면 도움을 받을 수 있다.\n",
        "\n",
        "자바홈은 압축이 풀리는 기준 jdk 폴더를 자바홈을 설정하므로 디렉토리를 jvm.dll이 들어있는 폴더로 재설정해주어야 한다.\n"
      ],
      "metadata": {
        "id": "jDObm413VMhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy\n"
      ],
      "metadata": {
        "id": "373w4dV-Vful",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37222841-f0d0-40d6-cb32-7ea2b0680a57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 로딩\n",
        "\n",
        "https://github.com/e9t/nsmc\n",
        "\n",
        "테스트 데이터가 별도로 있으니 이를 이용해 평가한다."
      ],
      "metadata": {
        "id": "RFzOyQH9VjWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/ESAA/OB/ratings_train.txt', sep='\\t')\n",
        "train_df.head(3)\n"
      ],
      "metadata": {
        "id": "T60t9zPGVt37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "4968d20e-6876-4e91-fcf8-367d11cc93b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                           document  label\n",
              "0   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                  너무재밓었다그래서보는것을추천한다      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-10840434-9baf-4f1f-b82a-f35f3d5c1087\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10840434-9baf-4f1f-b82a-f35f3d5c1087')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-10840434-9baf-4f1f-b82a-f35f3d5c1087 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-10840434-9baf-4f1f-b82a-f35f3d5c1087');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터세트의 0과 1라벨값 비교\n",
        "train_df['label'].value_counts()\n"
      ],
      "metadata": {
        "id": "v8CadWYOV2va",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b380f7e4-a9ef-4c31-e2ef-ad37a2927232"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    75173\n",
              "1    74827\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0과 1의 비율이 한쪽으로 치우치지 않고 균등한 분포를 나타낸다. train_df의 경우 리뷰텍스트를 갖는 document 칼럼에 null이 일부 존재하므로 이 값은 공백으로 변환한다. 문자가 아닌 숫자의 경우 단어적인 의미로 부족하므로 파이썬의 정규표현식 모듈인 re를이용해 공백으로 변환한다. 테스트 데이터세트도 동일하게 가공을 수행한다."
      ],
      "metadata": {
        "id": "OaN9_oo8V8x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "train_df = train_df.fillna(' ')\n",
        "# 정규 표현식을 이용해 숫자를 공백으로 변경)\n",
        "train_df['document'] = train_df['document'].apply(lambda x : re.sub(r'\\d+',' ', x))\n",
        "\n",
        "# loading test data set, transform null and num to ' '\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/ESAA/OB/ratings_test.txt', sep='\\t')\n",
        "test_df = test_df.fillna(' ')\n",
        "test_df['document']=test_df['document'].apply(lambda x:re.sub(r'\\d+', ' ',x))\n",
        "\n",
        "# delete id column\n",
        "train_df.drop('id', axis=1, inplace=True)\n",
        "test_df.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "84GNzyn4n1lt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tfidf 방식으로 단어 벡터화, 먼저 각 문장을 한글형태소 분석을 통해 형태소 단어로 토큰화한다. 한글 형태소 엔진은 sns 분석에 적합한 twitter 클래스를 이용한다. twitter 객체의 morphus 메서드를 이용시 입력인자로 들어온 문장을 형태소단어 형태로 토큰화해 list 객체로 변환한다. 문장을 형태소 단어로 반환하는 토크나이저 함수 tw tokenizer를 생성한다."
      ],
      "metadata": {
        "id": "PWjtb46ZbG0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Twitter\n",
        "twitter = Twitter()\n",
        "def tw_tokenizer(text):\n",
        "  # 입력인자로 들어온 텍스트를 형태소 단어로 토큰화해 리스트 형태로 반환\n",
        "  tokens_ko = twitter.morphs(text)\n",
        "  return tokens_ko"
      ],
      "metadata": {
        "id": "U3SrZAGrWKDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350f1698-9286-4e66-ba5c-f0756aa934eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/konlpy/tag/_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# twitter 객체의 morphs 객체를 이요한 tokenizer 사용, ngramrange는 1,2\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
        "tfidf_vect.fit(train_df['document'])\n",
        "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])"
      ],
      "metadata": {
        "id": "3qGLajFPbqrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression 감성분석\n",
        "lg_clf = LogisticRegression(random_state=0)\n",
        "\n",
        "#파라미터 c 최적화를 위해 gridsearch 를 이용\n",
        "params = {'C':[1,3.5,4.5, 5.5,10]}\n",
        "grid_cv = GridSearchCV(lg_clf, param_grid = params, cv=3, scoring='accuracy', verbose=1)\n",
        "grid_cv.fit(tfidf_matrix_train, train_df['label'])\n",
        "print(grid_cv.best_params_, round(grid_cv.best_score_,4))"
      ],
      "metadata": {
        "id": "w9Y5eh5ecOkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}